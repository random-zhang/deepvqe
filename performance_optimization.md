# DeepVQE-AEC è®­ç»ƒæ€§èƒ½ä¼˜åŒ–æŒ‡å—

## ğŸš€ ä¸»è¦æ€§èƒ½ç“¶é¢ˆåˆ†æ

### 1. æ•°æ®åŠ è½½ç“¶é¢ˆ (æœ€ä¸¥é‡)
- **é—®é¢˜**: `num_workers=0` å¯¼è‡´å•çº¿ç¨‹æ•°æ®åŠ è½½
- **å½±å“**: æ•°æ®åŠ è½½æˆä¸ºè®­ç»ƒçš„ä¸»è¦ç“¶é¢ˆï¼ŒGPUåˆ©ç”¨ç‡ä½
- **è§£å†³æ–¹æ¡ˆ**: è®¾ç½® `num_workers=4-8`

### 2. å®æ—¶STFTè®¡ç®—
- **é—®é¢˜**: æ¯ä¸ªæ ·æœ¬éƒ½å®æ—¶è®¡ç®—STFTï¼ŒCPUå¯†é›†
- **å½±å“**: æ¯ä¸ªepoché‡å¤ç›¸åŒçš„è®¡ç®—
- **è§£å†³æ–¹æ¡ˆ**: é¢„è®¡ç®—STFTå¹¶ç¼“å­˜

### 3. å†…å­˜ä¼ è¾“å¼€é”€
- **é—®é¢˜**: CPU-GPUæ•°æ®ä¼ è¾“é¢‘ç¹
- **å½±å“**: å¢åŠ è®­ç»ƒæ—¶é—´
- **è§£å†³æ–¹æ¡ˆ**: å¯ç”¨ `pin_memory=True`

## ğŸ› ï¸ å…·ä½“ä¼˜åŒ–å»ºè®®

### ç«‹å³å¯å®æ–½çš„ä¼˜åŒ– (ç®€å•)

#### 1. ä¿®æ”¹æ•°æ®åŠ è½½å™¨å‚æ•°
```python
# åœ¨ train_aec.py ä¸­ä¿®æ”¹ DataLoader é…ç½®
dl = DataLoader(
    ds, 
    batch_size=args.batch_size, 
    shuffle=True, 
    num_workers=4,  # æ”¹ä¸º4-8
    collate_fn=collate_fn, 
    drop_last=True,
    pin_memory=True,  # æ·»åŠ è¿™è¡Œ
    persistent_workers=True  # æ·»åŠ è¿™è¡Œ
)
```

#### 2. å¯ç”¨PyTorchä¼˜åŒ–
```python
# åœ¨è®­ç»ƒå¼€å§‹å‰æ·»åŠ 
if device.type == 'cuda':
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
```

#### 3. å‡å°‘éªŒè¯é¢‘ç‡
```python
# æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡ï¼Œè€Œä¸æ˜¯æ¯ä¸ªepoch
if epoch % 5 == 0:
    validate()
```

### ä¸­ç­‰éš¾åº¦ä¼˜åŒ–

#### 1. é¢„è®¡ç®—STFTç¼“å­˜
- ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶é¢„è®¡ç®—æ‰€æœ‰STFT
- åç»­è®­ç»ƒç›´æ¥åŠ è½½ç¼“å­˜
- å¯èŠ‚çœ50-70%çš„æ•°æ®é¢„å¤„ç†æ—¶é—´

#### 2. æ··åˆç²¾åº¦è®­ç»ƒ
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
with autocast():
    output = model(X_mic, X_far)
    loss = criterion(output, X_clean)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

#### 3. æ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–
```python
# å‡å°‘å®é™…batch_sizeï¼Œå¢åŠ accumulate_grad_batches
# ä¾‹å¦‚: batch_size=4, accumulate_grad_batches=4
# ç­‰æ•ˆäº batch_size=16ï¼Œä½†å†…å­˜ä½¿ç”¨æ›´å°‘
```

### é«˜çº§ä¼˜åŒ– (å¤æ‚)

#### 1. æ¨¡å‹ç¼–è¯‘ (PyTorch 2.0+)
```python
model = torch.compile(model)
```

#### 2. æ•°æ®é¢„å¤„ç†ä¼˜åŒ–
- ä½¿ç”¨GPUè¿›è¡ŒSTFTè®¡ç®—
- å®ç°è‡ªå®šä¹‰CUDA kernel
- ä½¿ç”¨TensorRTä¼˜åŒ–æ¨ç†

#### 3. åˆ†å¸ƒå¼è®­ç»ƒ
```python
# ä½¿ç”¨å¤šGPUè®­ç»ƒ
python -m torch.distributed.launch --nproc_per_node=2 train_aec.py
```

## ğŸ“Š é¢„æœŸæ€§èƒ½æå‡

| ä¼˜åŒ–é¡¹ç›® | é¢„æœŸæå‡ | å®æ–½éš¾åº¦ |
|---------|---------|---------|
| num_workers=4 | 2-4x | ç®€å• |
| pin_memory | 10-20% | ç®€å• |
| STFTç¼“å­˜ | 50-70% | ä¸­ç­‰ |
| æ··åˆç²¾åº¦ | 20-30% | ä¸­ç­‰ |
| æ¨¡å‹ç¼–è¯‘ | 10-15% | ç®€å• |
| åˆ†å¸ƒå¼è®­ç»ƒ | 1.8x (2GPU) | å¤æ‚ |

## ğŸ”§ å¿«é€Ÿä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: æœ€å°ä¿®æ”¹ (æ¨è)
åªéœ€ä¿®æ”¹ `train_aec.py` ä¸­çš„å‡ è¡Œä»£ç ï¼š

```python
# ç¬¬1æ­¥: ä¿®æ”¹DataLoader
num_workers = 4  # æˆ–è€… min(8, os.cpu_count())
dl = DataLoader(ds, batch_size=args.batch_size, shuffle=True, 
                num_workers=num_workers, collate_fn=collate_fn, 
                drop_last=True, pin_memory=True, persistent_workers=True)

# ç¬¬2æ­¥: å¯ç”¨cudnnä¼˜åŒ–
if device.type == 'cuda':
    torch.backends.cudnn.benchmark = True

# ç¬¬3æ­¥: å‡å°‘éªŒè¯é¢‘ç‡
if epoch % 5 == 0 and args.use_val:  # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
    validate()
```

### æ–¹æ¡ˆ2: ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬è„šæœ¬
ä½¿ç”¨æˆ‘åˆ›å»ºçš„ `train_aec_optimized.py`ï¼ŒåŒ…å«æ‰€æœ‰ä¼˜åŒ–ã€‚

## ğŸ¯ å»ºè®®çš„å®æ–½é¡ºåº

1. **ç«‹å³å®æ–½**: ä¿®æ”¹ `num_workers` å’Œ `pin_memory`
2. **çŸ­æœŸ**: å¯ç”¨cudnnä¼˜åŒ–ï¼Œå‡å°‘éªŒè¯é¢‘ç‡
3. **ä¸­æœŸ**: å®æ–½STFTç¼“å­˜
4. **é•¿æœŸ**: æ··åˆç²¾åº¦è®­ç»ƒï¼Œæ¨¡å‹ç¼–è¯‘

## ğŸ’¡ ç›‘æ§æ€§èƒ½

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç›‘æ§è®­ç»ƒæ€§èƒ½ï¼š
```bash
# ç›‘æ§GPUä½¿ç”¨ç‡
nvidia-smi -l 1

# ç›‘æ§CPUä½¿ç”¨ç‡
htop

# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ æ—¶é—´ç»Ÿè®¡
import time
start_time = time.time()
# ... è®­ç»ƒä»£ç  ...
print(f"Epoch time: {time.time() - start_time:.2f}s")
```

é€šè¿‡è¿™äº›ä¼˜åŒ–ï¼Œä½ çš„è®­ç»ƒé€Ÿåº¦åº”è¯¥èƒ½æå‡ **3-5å€**ï¼